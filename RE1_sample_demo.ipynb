{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33aa1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import tkinter\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "matplotlib.use('TkAgg')\n",
    "%matplotlib inline \n",
    "from PIL import Image\n",
    "\n",
    "import pyrealsense2 as rs                 # Intel RealSense cross-platform open-source API\n",
    "from stretch_body.robot import Robot\n",
    "from scipy.spatial.transform import Rotation\n",
    "from re1_utils.camera import get_cur_rs_frame, get_rs_colorized_depth\n",
    "from re1_utils.objdet_utils import plot_all_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844080a",
   "metadata": {},
   "source": [
    "**Object Detection**: From yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7527bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./yolov7/')\n",
    "from yolov7.predict import load_model, preprocessing, \\\n",
    "    postprocessing, load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32274f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [utils.torch_utils]: YOLOR ðŸš€ a0bbdb1 torch 1.12.1+cu102 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [utils.torch_utils]: Model Summary: 306 layers, 36905341 parameters, 36905341 gradients\n"
     ]
    }
   ],
   "source": [
    "#Load config from yaml\n",
    "CONFIG_PATH = './yolov7/predict_config.yaml'\n",
    "config_dict = load_config(CONFIG_PATH)\n",
    "\n",
    "#Load model, edit yolov7.pt path\n",
    "model, stride, device = load_model(config_dict, '../yolov7/yolov7.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0fce2",
   "metadata": {},
   "source": [
    "Read sample img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f1e0779",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        250          78         633         531     0.96753           0]\n",
      " [          2           5         402         527     0.95566           0]\n",
      " [        196         199         254         418     0.88187          27]\n",
      " [        256         207         299         247     0.42756          55]] (4, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hello-robot/anaconda3/envs/RE1/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "#Read img0 to BGR\n",
    "img0 = cv2.imread('./yolov7/inference/images/image1.jpg')\n",
    "# img0 = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
    "#Resize, pad\n",
    "img = preprocessing(config_dict, img0, stride, device) \n",
    "\n",
    "#Inference\n",
    "pred = model(img, augment=config_dict['augment'])[0] #Shape (1, num_preds, 85)\n",
    "\n",
    "#nms and scale coordinates\n",
    "pred = postprocessing(config_dict, pred, img0.shape, img.shape)\n",
    "\n",
    "#xyxy, conf_score, class\n",
    "print(pred, pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7c62c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img0 = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plot_all_boxes(pred, img0)\n",
    "# plt.imshow(img0)\n",
    "# plt.show()\n",
    "\n",
    "img = Image.fromarray(img0, 'RGB')\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3a15688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames Captured\n"
     ]
    }
   ],
   "source": [
    "color_frame, color, depth_frame, depth = get_cur_rs_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9429a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        219         200         384         469     0.94141          56]\n",
      " [        112         159         259         241     0.91639          62]\n",
      " [          1         263         194         638     0.75953          56]\n",
      " [        100         238         210         260     0.62988          66]\n",
      " [         79         204          98         256     0.57073          39]\n",
      " [        285         472         310         546     0.49452          39]\n",
      " [        350         416         376         491     0.49109          39]\n",
      " [        255         196         340         252     0.39538          63]] (8, 6)\n"
     ]
    }
   ],
   "source": [
    "from re1_utils.objdet_utils import plot_all_boxes\n",
    "\n",
    "#Resize, pad\n",
    "color_img = preprocessing(config_dict, color, stride, device) \n",
    "\n",
    "#Inference\n",
    "pred = model(color_img, augment=config_dict['augment'])[0] #Shape (1, num_preds, 85)\n",
    "\n",
    "#nms and scale coordinates\n",
    "pred = postprocessing(config_dict, pred, color.shape, color_img.shape)\n",
    "\n",
    "#xyxy, conf_score, class\n",
    "print(pred, pred.shape)\n",
    "color = np.ascontiguousarray(color, dtype=np.uint8)\n",
    "\n",
    "plot_all_boxes(pred, color)\n",
    "color_img = Image.fromarray(color, 'RGB')\n",
    "color_img.show()\n",
    "# plt.imshow(color)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9acba000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1273/1223485858.py:3: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "colorized_depth = get_rs_colorized_depth(depth_frame=depth_frame)\n",
    "plt.imshow(colorized_depth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1d5f08",
   "metadata": {},
   "source": [
    "**Testing landmark screen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b03b308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames Captured\n"
     ]
    }
   ],
   "source": [
    "from re1_utils.camera import get_cur_rs_frame\n",
    "color_frame, color, depth_frame, depth = get_cur_rs_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ddd7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chair at ((219, 200), (384, 469)) depth 2.814000129699707 meters\n",
      "tv at ((112, 159), (259, 241)) depth 3.0380001068115234 meters\n",
      "chair at ((1, 263), (194, 638)) depth 1.3450000286102295 meters\n",
      "keyboard at ((100, 238), (210, 260)) depth 3.1000001430511475 meters\n",
      "bottle at ((79, 204), (98, 256)) depth 3.069000244140625 meters\n",
      "bottle at ((285, 472), (310, 546)) depth 1.715000033378601 meters\n",
      "bottle at ((350, 416), (376, 491)) depth 2.0910000801086426 meters\n",
      "laptop at ((255, 196), (340, 252)) depth 3.1000001430511475 meters\n"
     ]
    }
   ],
   "source": [
    "from POI.landmark_screen import LandmarkScreen\n",
    "\n",
    "\n",
    "landmark_screen = LandmarkScreen(color_frame=color, depth_frame=depth)\n",
    "landmark_screen.update_OOI(pred)\n",
    "# landmark_screen.show()\n",
    "landmark_screen.show_OOI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb61ff96",
   "metadata": {},
   "source": [
    "**Test Intrinsic Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efc24488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     211.06           0      214.73]\n",
      " [          0      211.06      117.93]\n",
      " [          0           0           1]]\n",
      "[[   0.004738           0     -1.0174]\n",
      " [          0    0.004738    -0.55877]\n",
      " [          0           0           1]]\n"
     ]
    }
   ],
   "source": [
    "#Test intrinsic matrix\n",
    "from re1_utils.camera import get_rs_intrinsic_mat\n",
    "intrinsic_mat, _ = get_rs_intrinsic_mat()\n",
    "print(intrinsic_mat)\n",
    "inv_intrinsic_mat = np.linalg.inv(intrinsic_mat)\n",
    "print(inv_intrinsic_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6c3f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from POI.landmark_screen import LandmarkScreen\n",
    "\n",
    "landmark_screen.update_cam_coords(inv_intrinsic_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "185107a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyboard at ((100, 238), (210, 260)) depth 3.1000001430511475 meters\n",
      "bottle at ((285, 472), (310, 546)) depth 1.715000033378601 meters\n"
     ]
    }
   ],
   "source": [
    "keyboard = landmark_screen.get_OOI()[3]\n",
    "keyboard.show()\n",
    "closest_bottle = landmark_screen.get_OOI()[5]\n",
    "closest_bottle.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e3ed2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[         55          11           1] [    -2.3462     -1.5706         3.1]\n",
      "[       12.5          37           1] [    -1.6433    -0.65764       1.715]\n"
     ]
    }
   ],
   "source": [
    "print(keyboard.img_coord, keyboard.cam_coord)\n",
    "print(closest_bottle.img_coord, closest_bottle.cam_coord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f28a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c736343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from POI.object_of_interest import OOI\n",
    "all_objects = []\n",
    "for i in range(pred.shape[0]):\n",
    "    coord = pred[i, :]\n",
    "    obj = OOI(\n",
    "        img_coord = np.array([(coord[2]-coord[0])/2,(coord[3]-coord[1])/2]), \n",
    "        depth = 0,\n",
    "        obj_class = int(coord[-1]),\n",
    "        obj_atributes = 'None', \n",
    "        bbox = ((coord[0],coord[1]),(coord[2],coord[3])),\n",
    "        conf_score = coord[4],\n",
    "        eid = i\n",
    "    )\n",
    "    all_objects.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda906b6",
   "metadata": {},
   "source": [
    "MEMORY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa2488",
   "metadata": {},
   "source": [
    "*Taken from fairo tutorial and memory module\n",
    "\n",
    "\n",
    "**Memory**\n",
    "\n",
    "Now we have setup a small object detection + deduplication pipeline.\n",
    "\n",
    "But the robot is not storing the information of these objects in it's memory yet.\n",
    "\n",
    "If it doesn't store this information in memory, then when you say \"go to the chair\", it does not know where the chair is (unless the chair is in it's field of view at that given moment).\n",
    "\n",
    "`droidlet` provides a memory system that can store generic metadata. This memory system is used by the Dialog Parser + Task controller to do tasks utilizing context provided by information stored in memory.\n",
    "\n",
    "The memory is backed by an SQL database, and has schemas to represent common semantic information for robots and the environment.\n",
    "\n",
    "Let us first create the default `AgentMemory` object for our Locobot using some pre-baked and thoughtful memory schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc3168",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./memory/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f82b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from memory.sql_memory import AgentMemory\n",
    "from memory.robot.loco_memory_nodes import NODELIST\n",
    "\n",
    "SQL_SCHEMAS = [\n",
    "    os.path.join(os.getcwd(), \"memory\", \"base_memory_schema.sql\"),\n",
    "    os.path.join(os.getcwd(), \"memory\", \"robot\",\"loco_memory_schema.sql\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12fed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = AgentMemory(db_file=\":memory:\", schema_paths=SQL_SCHEMAS, nodelist=NODELIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0af1c4",
   "metadata": {},
   "source": [
    "We can see the types of nodes that can be stored inside the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a081bf",
   "metadata": {},
   "source": [
    "Let us store the previously detected objects into memory, using this new memory system.\n",
    "\n",
    "For this, we will use the DetectedObjectNode. A physical object is represented in memory as a DetectedObjectNode, which is thoughtfully annotated with properties such as it's color and it's detected xyz location.\n",
    "\n",
    "As a reminder, in the previous section, we deduplicated the objects detected in the scene, and stored them in the variable previous_objects.\n",
    "\n",
    "Now, we will store these all_objects into the memory. Let us start with storing and retreiving one object, and inspecting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b879b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from memory.robot.loco_memory_nodes import Detect\n",
    "from memory.robot.loco_memory_nodes import BCIDetectedObjectNode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_ids = []\n",
    "for i in range(len(all_objects)):\n",
    "    memory_id = BCIDetectedObjectNode.create(memory, all_objects[i])\n",
    "    memory_ids.append(memory_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde519b",
   "metadata": {},
   "source": [
    "Now, let us retreive the `DetectedObjectNode` from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.get_mem_by_id(memory_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d737cb33",
   "metadata": {},
   "source": [
    "The memory object is in it's raw packed form, and is not yet converted back to a dict with accessible fields.\n",
    "\n",
    "We can access the detected objects back from memory as dicts using the `get_all` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f345485",
   "metadata": {},
   "outputs": [],
   "source": [
    "BCIDetectedObjectNode.get_all(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa734f7b",
   "metadata": {},
   "source": [
    "Now, let us store the rest of the detected objects into memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('RE1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb71b73155e50e0471bb3f6aa8b6532acb978151faf9fdb589f3080a8ce410fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
