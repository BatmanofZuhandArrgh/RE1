{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33aa1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import tkinter\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "matplotlib.use('TkAgg')\n",
    "%matplotlib inline \n",
    "from PIL import Image\n",
    "\n",
    "import pyrealsense2 as rs                 # Intel RealSense cross-platform open-source API\n",
    "from stretch_body.robot import Robot\n",
    "from scipy.spatial.transform import Rotation\n",
    "from re1_utils.camera import get_cur_rs_frame, get_rs_colorized_depth\n",
    "from re1_utils.objdet_utils import plot_all_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53844361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./yolov7/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b506c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from yolov7.predict import load_model, preprocessing, \\\n",
    "    postprocessing, load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844080a",
   "metadata": {},
   "source": [
    "**Object Detection**: From yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32274f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [utils.torch_utils]: YOLOR ðŸš€ a0bbdb1 torch 1.12.1+cu102 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [utils.torch_utils]: Model Summary: 306 layers, 36905341 parameters, 36905341 gradients\n"
     ]
    }
   ],
   "source": [
    "#Load config from yaml\n",
    "CONFIG_PATH = './yolov7/predict_config.yaml'\n",
    "config_dict = load_config(CONFIG_PATH)\n",
    "\n",
    "#Load model, edit yolov7.pt path\n",
    "model, stride, device = load_model(config_dict, '../yolov7/yolov7.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0fce2",
   "metadata": {},
   "source": [
    "Read sample img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f1e0779",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        250          78         633         531     0.96753           0]\n",
      " [          2           5         402         527     0.95566           0]\n",
      " [        196         199         254         418     0.88187          27]\n",
      " [        256         207         299         247     0.42756          55]] (4, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hello-robot/anaconda3/envs/RE1/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "#Read img0 to BGR\n",
    "img0 = cv2.imread('./yolov7/inference/images/image1.jpg')\n",
    "# img0 = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
    "#Resize, pad\n",
    "img = preprocessing(config_dict, img0, stride, device) \n",
    "\n",
    "#Inference\n",
    "pred = model(img, augment=config_dict['augment'])[0] #Shape (1, num_preds, 85)\n",
    "\n",
    "#nms and scale coordinates\n",
    "pred = postprocessing(config_dict, pred, img0.shape, img.shape)\n",
    "\n",
    "#xyxy, conf_score, class\n",
    "print(pred, pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c62c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img0 = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plot_all_boxes(pred, img0)\n",
    "# plt.imshow(img0)\n",
    "# plt.show()\n",
    "\n",
    "img = Image.fromarray(img0, 'RGB')\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3a15688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames Captured\n"
     ]
    }
   ],
   "source": [
    "color_frame, color, depth_frame, depth = get_cur_rs_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9429a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        218         199         384         470     0.94106          56]\n",
      " [        112         159         258         240     0.90717          62]\n",
      " [          1         264         193         638     0.77426          56]\n",
      " [        100         238         210         260     0.65082          66]\n",
      " [        376         470         405         543     0.59137          39]\n",
      " [        284         479         309         543     0.47652          39]\n",
      " [        255         196         340         253     0.43999          63]\n",
      " [        350         416         376         491     0.36785          39]] (8, 6)\n"
     ]
    }
   ],
   "source": [
    "from re1_utils.objdet_utils import plot_all_boxes\n",
    "\n",
    "#Resize, pad\n",
    "color_img = preprocessing(config_dict, color, stride, device) \n",
    "\n",
    "#Inference\n",
    "pred = model(color_img, augment=config_dict['augment'])[0] #Shape (1, num_preds, 85)\n",
    "\n",
    "#nms and scale coordinates\n",
    "pred = postprocessing(config_dict, pred, color.shape, color_img.shape)\n",
    "\n",
    "#xyxy, conf_score, class\n",
    "print(pred, pred.shape)\n",
    "color = np.ascontiguousarray(color, dtype=np.uint8)\n",
    "\n",
    "plot_all_boxes(pred, color)\n",
    "color_img = Image.fromarray(color, 'RGB')\n",
    "color_img.show()\n",
    "# plt.imshow(color)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9acba000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27993/1223485858.py:3: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "colorized_depth = get_rs_colorized_depth(depth_frame=depth_frame)\n",
    "plt.imshow(colorized_depth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60ddd7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 1 1 1]\n",
      " [0 1 1 ... 1 1 0]\n",
      " [0 0 1 ... 1 0 0]]\n",
      "chair at ((218, 199), (384, 470)) depth 2.0 meters\n",
      "tv at ((112, 159), (258, 240)) depth 3.0 meters\n",
      "chair at ((1, 264), (193, 638)) depth 1.0 meters\n",
      "keyboard at ((100, 238), (210, 260)) depth 3.0 meters\n",
      "bottle at ((376, 470), (405, 543)) depth 1.0 meters\n",
      "bottle at ((284, 479), (309, 543)) depth 1.0 meters\n",
      "laptop at ((255, 196), (340, 253)) depth 3.0 meters\n",
      "bottle at ((350, 416), (376, 491)) depth 2.0 meters\n"
     ]
    }
   ],
   "source": [
    "from POI.landmark_screen import LandmarkScreen\n",
    "\n",
    "\n",
    "landmark_screen = LandmarkScreen(color_frame=color, depth_frame=depth)\n",
    "landmark_screen.update_OOI(pred)\n",
    "# landmark_screen.show()\n",
    "landmark_screen.show_OOI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efc24488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     211.06           0      214.73]\n",
      " [          0      211.06      117.93]\n",
      " [          0           0           1]]\n"
     ]
    }
   ],
   "source": [
    "#Test intrinsic matrix\n",
    "from POI.landmark_screen import LandmarkScreen\n",
    "from re1_utils.camera import get_rs_intrinsic_mat\n",
    "intrinsic_mat, _ = get_rs_intrinsic_mat()\n",
    "print(intrinsic_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185107a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3ed2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f28a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c736343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from POI.object_of_interest import OOI\n",
    "all_objects = []\n",
    "for i in range(pred.shape[0]):\n",
    "    coord = pred[i, :]\n",
    "    obj = OOI(\n",
    "        img_coord = np.array([(coord[2]-coord[0])/2,(coord[3]-coord[1])/2]), \n",
    "        depth = 0,\n",
    "        obj_class = int(coord[-1]),\n",
    "        obj_atributes = 'None', \n",
    "        bbox = ((coord[0],coord[1]),(coord[2],coord[3])),\n",
    "        conf_score = coord[4],\n",
    "        eid = i\n",
    "    )\n",
    "    all_objects.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda906b6",
   "metadata": {},
   "source": [
    "MEMORY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa2488",
   "metadata": {},
   "source": [
    "*Taken from fairo tutorial and memory module\n",
    "\n",
    "\n",
    "**Memory**\n",
    "\n",
    "Now we have setup a small object detection + deduplication pipeline.\n",
    "\n",
    "But the robot is not storing the information of these objects in it's memory yet.\n",
    "\n",
    "If it doesn't store this information in memory, then when you say \"go to the chair\", it does not know where the chair is (unless the chair is in it's field of view at that given moment).\n",
    "\n",
    "`droidlet` provides a memory system that can store generic metadata. This memory system is used by the Dialog Parser + Task controller to do tasks utilizing context provided by information stored in memory.\n",
    "\n",
    "The memory is backed by an SQL database, and has schemas to represent common semantic information for robots and the environment.\n",
    "\n",
    "Let us first create the default `AgentMemory` object for our Locobot using some pre-baked and thoughtful memory schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc3168",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./memory/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f82b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from memory.sql_memory import AgentMemory\n",
    "from memory.robot.loco_memory_nodes import NODELIST\n",
    "\n",
    "SQL_SCHEMAS = [\n",
    "    os.path.join(os.getcwd(), \"memory\", \"base_memory_schema.sql\"),\n",
    "    os.path.join(os.getcwd(), \"memory\", \"robot\",\"loco_memory_schema.sql\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12fed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = AgentMemory(db_file=\":memory:\", schema_paths=SQL_SCHEMAS, nodelist=NODELIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0af1c4",
   "metadata": {},
   "source": [
    "We can see the types of nodes that can be stored inside the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a081bf",
   "metadata": {},
   "source": [
    "Let us store the previously detected objects into memory, using this new memory system.\n",
    "\n",
    "For this, we will use the DetectedObjectNode. A physical object is represented in memory as a DetectedObjectNode, which is thoughtfully annotated with properties such as it's color and it's detected xyz location.\n",
    "\n",
    "As a reminder, in the previous section, we deduplicated the objects detected in the scene, and stored them in the variable previous_objects.\n",
    "\n",
    "Now, we will store these all_objects into the memory. Let us start with storing and retreiving one object, and inspecting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b879b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from memory.robot.loco_memory_nodes import Detect\n",
    "from memory.robot.loco_memory_nodes import BCIDetectedObjectNode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_ids = []\n",
    "for i in range(len(all_objects)):\n",
    "    memory_id = BCIDetectedObjectNode.create(memory, all_objects[i])\n",
    "    memory_ids.append(memory_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde519b",
   "metadata": {},
   "source": [
    "Now, let us retreive the `DetectedObjectNode` from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.get_mem_by_id(memory_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d737cb33",
   "metadata": {},
   "source": [
    "The memory object is in it's raw packed form, and is not yet converted back to a dict with accessible fields.\n",
    "\n",
    "We can access the detected objects back from memory as dicts using the `get_all` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f345485",
   "metadata": {},
   "outputs": [],
   "source": [
    "BCIDetectedObjectNode.get_all(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa734f7b",
   "metadata": {},
   "source": [
    "Now, let us store the rest of the detected objects into memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('RE1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb71b73155e50e0471bb3f6aa8b6532acb978151faf9fdb589f3080a8ce410fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
