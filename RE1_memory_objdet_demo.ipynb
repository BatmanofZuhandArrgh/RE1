{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33aa1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import tkinter\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "matplotlib.use('TkAgg')\n",
    "%matplotlib inline \n",
    "from PIL import Image\n",
    "\n",
    "import pyrealsense2 as rs                 # Intel RealSense cross-platform open-source API\n",
    "from stretch_body.robot import Robot\n",
    "from scipy.spatial.transform import Rotation\n",
    "from re1_utils.camera import get_cur_rs_frame\n",
    "from re1_utils.objdet_utils import plot_all_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844080a",
   "metadata": {},
   "source": [
    "**Object Detection**: From yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7527bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./yolov7/')\n",
    "from yolov7.predict import load_model, preprocessing, \\\n",
    "    postprocessing, load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32274f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [utils.torch_utils]: YOLOR ðŸš€ a0bbdb1 torch 1.12.1+cu102 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [utils.torch_utils]: Model Summary: 306 layers, 36905341 parameters, 36905341 gradients\n"
     ]
    }
   ],
   "source": [
    "#Load config from yaml\n",
    "CONFIG_PATH = './yolov7/predict_config.yaml'\n",
    "config_dict = load_config(CONFIG_PATH)\n",
    "\n",
    "#Load model, edit yolov7.pt path\n",
    "model, stride, device = load_model(config_dict, '../yolov7/yolov7.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0fce2",
   "metadata": {},
   "source": [
    "Read sample img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f1e0779",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        250          78         633         531     0.96753           0]\n",
      " [          2           5         402         527     0.95566           0]\n",
      " [        196         199         254         418     0.88187          27]\n",
      " [        256         207         299         247     0.42756          55]] (4, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hello-robot/anaconda3/envs/RE1/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "#Read img0 to BGR\n",
    "img0 = cv2.imread('./yolov7/inference/images/image1.jpg')\n",
    "# img0 = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
    "#Resize, pad\n",
    "img = preprocessing(config_dict, img0, stride, device) \n",
    "\n",
    "#Inference\n",
    "pred = model(img, augment=config_dict['augment'])[0] #Shape (1, num_preds, 85)\n",
    "\n",
    "#nms and scale coordinates\n",
    "pred = postprocessing(config_dict, pred, img0.shape, img.shape)\n",
    "\n",
    "#xyxy, conf_score, class\n",
    "print(pred, pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c62c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img0 = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plot_all_boxes(pred, img0)\n",
    "# plt.imshow(img0)\n",
    "# plt.show()\n",
    "\n",
    "img = Image.fromarray(img0, 'RGB')\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3a15688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames Captured\n"
     ]
    }
   ],
   "source": [
    "color_frame, color, depth_frame, depth = get_cur_rs_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9429a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        121         342         296         628     0.89368          56]\n",
      " [        351         398         456         595     0.88076          56]\n",
      " [        243         303         380         373     0.81538          62]\n",
      " [        221         521         250         610     0.45188          39]\n",
      " [        402         345         421         394     0.42215          39]\n",
      " [        291         375         401         398     0.36896          66]\n",
      " [        402         345         421         394     0.36451          41]] (7, 6)\n"
     ]
    }
   ],
   "source": [
    "from re1_utils.objdet_utils import plot_all_boxes\n",
    "\n",
    "#Resize, pad\n",
    "color_img = preprocessing(config_dict, color, stride, device) \n",
    "\n",
    "#Inference\n",
    "pred = model(color_img, augment=config_dict['augment'])[0] #Shape (1, num_preds, 85)\n",
    "\n",
    "#nms and scale coordinates\n",
    "pred = postprocessing(config_dict, pred, color.shape, color_img.shape)\n",
    "\n",
    "#xyxy, conf_score, class\n",
    "print(pred, pred.shape)\n",
    "color = np.ascontiguousarray(color, dtype=np.uint8)\n",
    "\n",
    "plot_all_boxes(pred, color)\n",
    "color_img = Image.fromarray(color, 'RGB')\n",
    "color_img.show()\n",
    "# plt.imshow(color)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9acba000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re1_utils.camera import get_rs_colorized_depth\n",
    "colorized_depth = get_rs_colorized_depth(depth_frame=depth_frame)\n",
    "depth_img = Image.fromarray(colorized_depth, 'RGB')\n",
    "depth_img.show()\n",
    "# plt.imshow(colorized_depth)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1d5f08",
   "metadata": {},
   "source": [
    "**Testing landmark screen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b03b308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames Captured\n"
     ]
    }
   ],
   "source": [
    "from re1_utils.camera import get_cur_rs_frame\n",
    "color_frame, color, depth_frame, depth = get_cur_rs_frame()\n",
    "color = np.ascontiguousarray(color, dtype=np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60ddd7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chair at ((121, 342), (296, 628)) depth 2.690000057220459 meters\n",
      "chair at ((351, 398), (456, 595)) depth 3.319000244140625 meters\n",
      "tv at ((243, 303), (380, 373)) depth 3.132000207901001 meters\n",
      "bottle at ((221, 521), (250, 610)) depth 1.8850001096725464 meters\n",
      "bottle at ((402, 345), (421, 394)) depth 3.319000244140625 meters\n",
      "keyboard at ((291, 375), (401, 398)) depth 3.069000244140625 meters\n",
      "cup at ((402, 345), (421, 394)) depth 3.319000244140625 meters\n"
     ]
    }
   ],
   "source": [
    "from POI.landmark_screen import LandmarkScreen\n",
    "\n",
    "\n",
    "landmark_screen = LandmarkScreen(color_frame=color, depth_frame=depth)\n",
    "landmark_screen.update_OOI(pred)\n",
    "landmark_screen.show()\n",
    "landmark_screen.show_OOI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb61ff96",
   "metadata": {},
   "source": [
    "**Test Intrinsic Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efc24488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     606.46           0      323.98]\n",
      " [          0       606.4      234.81]\n",
      " [          0           0           1]]\n",
      "[[  0.0016489           0     -0.5342]\n",
      " [          0   0.0016491    -0.38722]\n",
      " [          0           0           1]]\n"
     ]
    }
   ],
   "source": [
    "#Test intrinsic matrix\n",
    "from re1_utils.camera import get_rs_intrinsic_mat\n",
    "intrinsic_mat = get_rs_intrinsic_mat()\n",
    "print(intrinsic_mat)\n",
    "inv_intrinsic_mat = np.linalg.inv(intrinsic_mat)\n",
    "print(inv_intrinsic_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6c3f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from POI.landmark_screen import LandmarkScreen\n",
    "\n",
    "landmark_screen.update_cam_coords(inv_intrinsic_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "185107a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyboard at ((291, 375), (401, 398)) depth 3.069000244140625 meters\n",
      "bottle at ((221, 521), (250, 610)) depth 1.8850001096725464 meters\n"
     ]
    }
   ],
   "source": [
    "#Please edit index\n",
    "keyboard = landmark_screen.get_OOI()[5]\n",
    "keyboard.show()\n",
    "closest_bottle = landmark_screen.get_OOI()[3]\n",
    "closest_bottle.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e3ed2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[        346       386.5           1] [    0.11146     0.76769       3.069]\n",
      "[      235.5       565.5           1] [     -0.275      1.0279       1.885]\n"
     ]
    }
   ],
   "source": [
    "print(keyboard.img_coord, keyboard.cam_coord)\n",
    "print(closest_bottle.img_coord, closest_bottle.cam_coord)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5061c8a9",
   "metadata": {},
   "source": [
    "**Test extrinsic matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd4d54d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "[[    0.99917   -0.036799    0.017461    0.012127]\n",
      " [   0.018093    0.016906    -0.99969    0.016835]\n",
      " [   0.036493     0.99918    0.017558      1.2441]\n",
      " [          0           0           0           1]]\n",
      "[[    0.99917    0.018093    0.036493   -0.057821]\n",
      " [  -0.036799    0.016906     0.99918     -1.2429]\n",
      " [   0.017461    -0.99969    0.017558  -0.0052257]\n",
      " [          0           0           0           1]]\n"
     ]
    }
   ],
   "source": [
    "#Test intrinsic matrix\n",
    "from re1_utils.camera import get_rs_extrinsic_mat\n",
    "extrinsic_mat = get_rs_extrinsic_mat(type = 'cam2base')\n",
    "print(extrinsic_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1634c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from POI.landmark_screen import LandmarkScreen\n",
    "\n",
    "landmark_screen.update_base_coords(extrinsic_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8501f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0.11146     0.76769       3.069] [    0.14883     -3.0362      2.0691]\n",
      "[     -0.275      1.0279       1.885] [   -0.26756     -1.8552      2.2942]\n"
     ]
    }
   ],
   "source": [
    "print(keyboard.cam_coord, keyboard.base_coord)\n",
    "print(closest_bottle.cam_coord, closest_bottle.base_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f28a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c736343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from POI.object_of_interest import OOI\n",
    "all_objects = []\n",
    "for i in range(pred.shape[0]):\n",
    "    coord = pred[i, :]\n",
    "    obj = OOI(\n",
    "        img_coord = np.array([(coord[2]-coord[0])/2,(coord[3]-coord[1])/2]), \n",
    "        depth = 0,\n",
    "        obj_class = int(coord[-1]),\n",
    "        obj_atributes = 'None', \n",
    "        bbox = ((coord[0],coord[1]),(coord[2],coord[3])),\n",
    "        conf_score = coord[4],\n",
    "        eid = i\n",
    "    )\n",
    "    all_objects.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda906b6",
   "metadata": {},
   "source": [
    "MEMORY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa2488",
   "metadata": {},
   "source": [
    "*Taken from fairo tutorial and memory module\n",
    "\n",
    "\n",
    "**Memory**\n",
    "\n",
    "Now we have setup a small object detection + deduplication pipeline.\n",
    "\n",
    "But the robot is not storing the information of these objects in it's memory yet.\n",
    "\n",
    "If it doesn't store this information in memory, then when you say \"go to the chair\", it does not know where the chair is (unless the chair is in it's field of view at that given moment).\n",
    "\n",
    "`droidlet` provides a memory system that can store generic metadata. This memory system is used by the Dialog Parser + Task controller to do tasks utilizing context provided by information stored in memory.\n",
    "\n",
    "The memory is backed by an SQL database, and has schemas to represent common semantic information for robots and the environment.\n",
    "\n",
    "Let us first create the default `AgentMemory` object for our Locobot using some pre-baked and thoughtful memory schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc3168",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./memory/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f82b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from memory.sql_memory import AgentMemory\n",
    "from memory.robot.loco_memory_nodes import NODELIST\n",
    "\n",
    "SQL_SCHEMAS = [\n",
    "    os.path.join(os.getcwd(), \"memory\", \"base_memory_schema.sql\"),\n",
    "    os.path.join(os.getcwd(), \"memory\", \"robot\",\"loco_memory_schema.sql\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12fed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = AgentMemory(db_file=\":memory:\", schema_paths=SQL_SCHEMAS, nodelist=NODELIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0af1c4",
   "metadata": {},
   "source": [
    "We can see the types of nodes that can be stored inside the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a081bf",
   "metadata": {},
   "source": [
    "Let us store the previously detected objects into memory, using this new memory system.\n",
    "\n",
    "For this, we will use the DetectedObjectNode. A physical object is represented in memory as a DetectedObjectNode, which is thoughtfully annotated with properties such as it's color and it's detected xyz location.\n",
    "\n",
    "As a reminder, in the previous section, we deduplicated the objects detected in the scene, and stored them in the variable previous_objects.\n",
    "\n",
    "Now, we will store these all_objects into the memory. Let us start with storing and retreiving one object, and inspecting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b879b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from memory.robot.loco_memory_nodes import Detect\n",
    "from memory.robot.loco_memory_nodes import BCIDetectedObjectNode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_ids = []\n",
    "for i in range(len(all_objects)):\n",
    "    memory_id = BCIDetectedObjectNode.create(memory, all_objects[i])\n",
    "    memory_ids.append(memory_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde519b",
   "metadata": {},
   "source": [
    "Now, let us retreive the `DetectedObjectNode` from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.get_mem_by_id(memory_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d737cb33",
   "metadata": {},
   "source": [
    "The memory object is in it's raw packed form, and is not yet converted back to a dict with accessible fields.\n",
    "\n",
    "We can access the detected objects back from memory as dicts using the `get_all` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f345485",
   "metadata": {},
   "outputs": [],
   "source": [
    "BCIDetectedObjectNode.get_all(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa734f7b",
   "metadata": {},
   "source": [
    "Now, let us store the rest of the detected objects into memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('3.7.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "79b1328d3e7af2c143ede97c186fcfa3b813c9249ef0bf8aa6d6406eca44b2bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
